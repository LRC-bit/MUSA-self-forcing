{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65a5295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vae import TAEHVDiffusersWrapper\n",
    "from text_encoder import WanTextEncoder\n",
    "from diffusion import WanDiffusionWrapper\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "from einops import rearrange\n",
    "from torchvision.io import write_video\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "config = OmegaConf.load('/mnt/data0/lab408/linruichen/Self-Forcing/configs/self_forcing_dmd.yaml')\n",
    "default_config = OmegaConf.load(\"/mnt/data0/lab408/linruichen/Self-Forcing/configs/default_config.yaml\")\n",
    "config = OmegaConf.merge(default_config, config)\n",
    "\n",
    "device = torch.device(\"musa:0\")\n",
    "\n",
    "current_vae = TAEHVDiffusersWrapper().to(device=device)\n",
    "current_vae = current_vae.eval()\n",
    "current_vae.to(dtype=torch.bfloat16)\n",
    "current_vae.requires_grad_(False)\n",
    "\n",
    "current_text_encoder = WanTextEncoder().to(device=device)\n",
    "\n",
    "current_generator = WanDiffusionWrapper(**getattr(config, \"model_kwargs\", {}))\n",
    "state_dict = torch.load('/mnt/data0/lab408/linruichen/Self-Forcing/checkpoints/self_forcing_dmd.pt', map_location=device)\n",
    "current_generator.load_state_dict(state_dict['generator_ema'])\n",
    "current_generator = current_generator.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58c7b6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV inference with 3 frames per block\n"
     ]
    }
   ],
   "source": [
    "from causal_inference import CausalInferencePipeline\n",
    "\n",
    "causal_pipeline = CausalInferencePipeline(\n",
    "    args=config,\n",
    "    device=device,\n",
    "    generator=current_generator,\n",
    "    text_encoder=current_text_encoder,\n",
    "    vae=current_vae\n",
    ")\n",
    "\n",
    "causal_pipeline = causal_pipeline.to(dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa3b5ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_timestep: 1000.0\n",
      "current_timestep: 937.5\n",
      "current_timestep: 833.3333129882812\n",
      "current_timestep: 625.0\n",
      "current_timestep: 1000.0\n",
      "current_timestep: 937.5\n",
      "current_timestep: 833.3333129882812\n",
      "current_timestep: 625.0\n",
      "current_timestep: 1000.0\n",
      "current_timestep: 937.5\n",
      "current_timestep: 833.3333129882812\n",
      "current_timestep: 625.0\n",
      "current_timestep: 1000.0\n",
      "current_timestep: 937.5\n",
      "current_timestep: 833.3333129882812\n",
      "current_timestep: 625.0\n",
      "current_timestep: 1000.0\n",
      "current_timestep: 937.5\n",
      "current_timestep: 833.3333129882812\n",
      "current_timestep: 625.0\n",
      "current_timestep: 1000.0\n",
      "current_timestep: 937.5\n",
      "current_timestep: 833.3333129882812\n",
      "current_timestep: 625.0\n",
      "current_timestep: 1000.0\n",
      "current_timestep: 937.5\n",
      "current_timestep: 833.3333129882812\n",
      "current_timestep: 625.0\n"
     ]
    }
   ],
   "source": [
    "num_output_frames=21\n",
    "num_samples=1\n",
    "\n",
    "all_video = []\n",
    "prompts = ['小男孩坐在书桌前'] * num_samples\n",
    "\n",
    "initial_latent = None\n",
    "sampled_noise = torch.randn([num_samples, num_output_frames, 16, 60, 104], device=device, dtype=torch.bfloat16)\n",
    "\n",
    "# Generate 81 frames\n",
    "video, latents = causal_pipeline.inference(\n",
    "    noise=sampled_noise,\n",
    "    text_prompts=prompts,\n",
    "    return_latents=True,\n",
    "    initial_latent=initial_latent\n",
    ")\n",
    "current_video = rearrange(video, 'b t c h w -> b t h w c').cpu()\n",
    "all_video.append(current_video)\n",
    "\n",
    "# Final output video\n",
    "video = 255.0 * torch.cat(all_video, dim=1)\n",
    "\n",
    "os.makedirs('/mnt/data0/lab408/linruichen/Self-Forcing/videos', exist_ok=True)\n",
    "for seed_idx in range(num_samples):    # All processes save their videos\n",
    "    output_path = os.path.join('/mnt/data0/lab408/linruichen/Self-Forcing/videos', f'{seed_idx}_ema.mp4')\n",
    "    write_video(output_path, video[seed_idx], fps=16)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YMclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
